{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données \n",
    "df = pd.read_csv('gapminder_data_graphs.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'on a 3 colonnes nous permettant de classer les données (pays, continent, année) ainsi que 5 colonnes contenant des valeurs numériques (espérance de vie, Indice de Développement Humain, consommation de CO2, Produit Intérieur Brut par habitant et pourcentage de travailleurs dans le secteur tertiaire). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde les valeurs manquantes\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(df.isnull(), cbar=False , cmap = 'magma')\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des villes et années avec valeurs d'IDH manquantes\n",
    "df[df['hdi_index'].isna()][['country', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est cohérent que plusieurs valeurs pour l'indice de développement humain soient manquantes dans le cas de certains pays car il n'était probablement possible de les calculer à l'époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des villes et années avec les valeurs de PIB manquantes\n",
    "df[df['gdp'].isna()][['country', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le même cas que précédemment, ces pays manquent de données pour le Produit Brut Intérieur car les données n'étaient probablement pas disponibles à l'époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des villes et années avec les valeurs de consommation de CO2 manquantes\n",
    "df[df['co2_consump'].isna()][['country', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant 2002, le Timor oriental était une province de l'Indonésie. Nous n'avons donc pas de données pour cette période."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes avec des valeurs manquantes\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "# Vérification\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde si on a des données dupliquées dans notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {}
   "outputs": [],
   "source": [
    "# On regarde si on a des données dupliquées\n",
    "print(\"Nombre de données dupliquées : \" , df.duplicated().sum())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On finit par modifier les index des données pour ne pas avoir de valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse descriptive ( moyenne, médiane, écart-type, histogrammes, boxplots, graphiques de lignes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyenne des données par pays\n",
    "df_without_year = df.drop('year', axis = 1)\n",
    "df_mean = df_without_year.groupby('country').mean(numeric_only=True)\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moyenne la plus élevée pour chaque colonne\n",
    "print(\"Pays avec les moyennes les plus hautes : \\n\", df_mean.idxmax())\n",
    "print(\"Pays avec les moyenne minimum : \\n\",df_mean.idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médianne des données par pays\n",
    "df_median = df_without_year.groupby('country').median(numeric_only=True)\n",
    "print(df_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme comparaison des varibles par continent\n",
    "fig, ax = plt.subplots(5, 1, figsize = (10, 20))\n",
    "\n",
    "sns.histplot(data = df_mean, x = 'life_exp', hue = 'continent', ax = ax[0], kde = True)\n",
    "sns.histplot(data = df_mean, x = 'hdi_index', hue = 'continent', ax = ax[1], kde = True)\n",
    "sns.histplot(data = df_mean, x = 'co2_consump', hue = 'continent', ax = ax[2], kde = True)\n",
    "sns.histplot(data = df_mean, x = 'gdp', hue = 'continent', ax = ax[3], kde = True)\n",
    "sns.histplot(data = df_mean, x = 'services', hue = 'continent', ax = ax[4], kde = True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot comparaison des varibles par continent\n",
    "fig, ax = plt.subplots(5, 1, figsize = (10, 20))\n",
    "sns.boxplot(data = df, x = 'services', y = 'continent', ax = ax[0], hue='continent')\n",
    "sns.boxplot(data = df, x = 'life_exp', y = 'continent', ax = ax[1], hue='continent')\n",
    "sns.boxplot(data = df, x = 'hdi_index', y = 'continent', ax = ax[2], hue='continent')\n",
    "sns.boxplot(data = df, x = 'gdp', y = 'continent', ax = ax[3], hue='continent')\n",
    "sns.boxplot(data = df, x = 'co2_consump', y = 'continent', ax = ax[4], hue='continent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique en barres de la consommation de CO2 par continent\n",
    "fig = px.bar(df, x = 'continent', y = 'co2_consump', color = 'country', title = 'Consommation de CO2 par continent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df, x = 'continent', y = 'life_exp', color = 'continent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df, x = 'continent', y = 'services', color = 'continent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df, x = 'continent', y = 'hdi_index', color = 'continent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df, x = 'continent', y = 'gdp', color = 'continent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrélation entre les colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va regarder quelles sont les colonnes qui sont corrélées entre elles pour voir si on peut en déduire quelque chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fonction qui affiche les corrélations entre les colonnes sur une heatmap\n",
    "'''\n",
    "def correlation(data):\n",
    "    correlation = data.corr()\n",
    "    plt.figure(figsize = (8,6))\n",
    "    sns.heatmap(correlation, annot = True, cmap = 'coolwarm')\n",
    "\n",
    "df_num = df.select_dtypes(include = ['float64'])\n",
    "correlation(df_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les valeurs des colonnes sont plutôt liées mais on voit notamment une forte corrélation entre l'IDH et l'espérance de vie ce qui est logique car l'IDH est calculé en prenant en compte l'espérance de vie. On observe également une corrélation importante entre l'IDH et le pourcentage de travailleurs dans le secteur tertiaire qui peut s'expliquer par le fait que les pays avec un IDH élevé ont souvent une économie tertiaire développée comme avec le tourisme par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des relations entre les colonnes\n",
    "liste_colonnes = [\"continent\", \"life_exp\", \"hdi_index\", \"co2_consump\", \"gdp\", \"services\"]\n",
    "sns.pairplot(data=df.loc[:, liste_colonnes], hue=\"continent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces différents graphiques illustrent bien les corrélations plus ou moins fortes entre les différentes colonnes. Les graphiques avec des points désordonnés et dispersés montrent une faible corrélation entre les colonnes tandis que les graphiques avec des points qui suivent approximativement une droite montrent une corrélation plus forte.\n",
    "\n",
    "On peut aussi essayer de regarder la corrélation entre les variables numériques et les différents continents pour voir si on peut en déduire quelque chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ajoute une variable pour chaque continent avec un booléen pour savoir si le pays est dans ce continent\n",
    "df_variables_continents = pd.get_dummies(df, columns = ['continent'])\n",
    "df_variables_continents = df_variables_continents.select_dtypes(include = ['float64', 'bool'])\n",
    "\n",
    "# Corrélation entre les variables\n",
    "correlation(df_variables_continents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut s'apercevoir ici que les coefficients de corrélation entre nos variables et les continents sont très différents selon les continents. Ceux-ci sont vont de 0.23 à 0.53 pour l'Europe tandis qu'ils sont situés entre -0.69 et -0.35 pour l'Afrique. Il est possible d'expliquer cela par le fait que l'Europe est un continent développé et possède donc des valeurs élevées pour l'IDH, le PIB par habitant ou l'espérance de vie alors que l'Afrique est un continent en développement et possède donc des valeurs plus faibles pour ces mêmes variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set() # Pour faire des modélisations plus jolies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va commencer par normaliser les données car les variables sont sur des échelles différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "df_normalise = scaler.fit_transform(df_num)\n",
    "df_normalise = pd.DataFrame(df_normalise, columns=df_num.columns, index=df_num.index) # Dataframe normalisé sans les colonnes catégorielles\n",
    "df_complet_normalise = df.copy()\n",
    "df_complet_normalise[df_num.columns] = df_normalise # Dataframe normalisé avec toutes les colonnes\n",
    "\n",
    "'''\n",
    "Méthode qui permer de dénormaliser les données\n",
    "'''\n",
    "def denormalisation(data):\n",
    "    df_denormalise = scaler.inverse_transform(data[df_num.columns])\n",
    "    df_denormalise = pd.DataFrame(df_denormalise, columns=df_num.columns)\n",
    "    df_complet_denormalise = df.copy()\n",
    "    df_complet_denormalise[df_num.columns] = df_denormalise\n",
    "    return df_complet_denormalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ensuite essayer de trouver le nombre de clusters optimal pour notre jeu de données. Pour cela, on utiliser la méthode du coude et ne prendre en compte que les colonnes numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Méthode qui permet de trouver le meilleur nombre de clusters pour KMeans\n",
    "'''\n",
    "def meilleur_nombre_classe_kmeans(df):\n",
    "    inerties = []\n",
    "    nombre_clusters = range(1, 15) \n",
    "    \n",
    "    # On applique KMeans pour chaque nombre de clusters\n",
    "    for k in nombre_clusters:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(df)\n",
    "        inerties.append(kmeans.inertia_)\n",
    "    \n",
    "    # On affiche la courbe d'inertie\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(nombre_clusters, inerties, marker='o')\n",
    "    plt.title(\"Courbe d'inertie\")\n",
    "    plt.xlabel('Nombre de clusters')\n",
    "    plt.ylabel('Inertie')\n",
    "    plt.xticks(nombre_clusters)\n",
    "    plt.show()\n",
    "    \n",
    "meilleur_nombre_classe_kmeans(df_normalise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit avec l'utilisation de la méthode du coude que le nombre optimal de clusters est de 3. On peut donc essayer de faire un clustering avec 3 clusters pour voir si on peut dégager quelque chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans avec 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df_normalise)\n",
    "\n",
    "# On dénormalise les données\n",
    "df_kmeans = denormalisation(df_normalise)\n",
    "df_kmeans = df_kmeans.select_dtypes(include = ['float64'])\n",
    "\n",
    "# On ajoute les clusters au dataframe pour les afficher\n",
    "labels = kmeans.labels_\n",
    "df_kmeans[\"cluster\"] = labels\n",
    "sns.pairplot(df_kmeans, hue=\"cluster\", palette=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde pour chaque continent la répartition des clusters\n",
    "df_kmeans[\"continent\"] = df[\"continent\"]\n",
    "print(df_kmeans.groupby([\"continent\", \"cluster\"]).size())\n",
    "df_kmeans.groupby([\"continent\", \"cluster\"]).size().unstack().plot(kind='bar', stacked=True, colormap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici la répartition des données pour chaque cluster dans les différents continents. On remarque clairement que les clusters ne sont pas répartis de la même manière selon les continents. En effet, les pays africains sont majoritairement dans le cluster 0 alors qu'aucun d'entre eux ne se trouve dans le cluster 2. Au contraire, les pays européens sont exclusivement dans les clusters 1 et 2. On peut donc en déduire que ces clusters correspondent à des niveaux de développement différents et que le premier correspond à des pays moins développés tandis que les deux autres correspondent à des pays plus développés.\n",
    "\n",
    "On va essayer de faire un clustering avec DBSCAN pour voir si on a des résultats similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste différentes valeurs de epsilon pour DBSCAN\n",
    "def test_epsilon_dbscan(df):   \n",
    "    for eps in np.arange(0.2, 2, 0.1):\n",
    "        db = DBSCAN(eps=eps, min_samples=3).fit(df)\n",
    "        labels = db.labels_\n",
    "        # On regarde si on a plus d'un cluster\n",
    "        if len(set(labels) - {-1}) > 1:\n",
    "            score = silhouette_score(df, labels)\n",
    "            print(f\"eps={eps:.2f}, Score de silhouette={score:.2f}\")\n",
    "    \n",
    "test_epsilon_dbscan(df_normalise)\n",
    "\n",
    "# On applique DBSCAN avec epsilon=0.9 car c'est la valeur qui donne le meilleur score de silhouette\n",
    "dbscan = DBSCAN(eps=0.9, min_samples=3)\n",
    "clusters = dbscan.fit_predict(df_normalise)\n",
    "\n",
    "# On dénormalise les données\n",
    "df_dbscan = denormalisation(df_normalise)\n",
    "df_dbscan = df_kmeans.select_dtypes(include = ['float64'])\n",
    "\n",
    "# On ajoute les clusters au dataframe pour les afficher\n",
    "labels = dbscan.labels_\n",
    "df_dbscan[\"clusters\"] = labels\n",
    "sns.pairplot(df_dbscan, hue=\"clusters\", palette=\"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats obtenus avec DBScan ne sont pas très concluants car on a une grande majorité de points dans le même cluster et seuls les points qui sont très éloignés des autres sont dans un autre cluster. On peut néanmoins s'intéresser à ces points pour voir si on peut en déduire quelque chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde pour chaque cluster les pays qu'il contient\n",
    "df_dbscan[\"country\"] = df[\"country\"]\n",
    "df_dbscan.groupby([\"clusters\", \"country\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque plusieurs choses intéressantes. Tout d'abord, on a la présence d'Haiti dans un cluster à part et lorsque l'on regarde les valeurs de ses colonnes, on observe qu'en 2010, l'espérance de vie descend à 32.5 ans avant de remonter l'année suivante. En faisant quelques recherches, on constate que cette baisse est due à un tremblement de terre qui a eu lieu en 2010 et qui a causé la mort de plusieurs centaines de milliers de personnes et donc une baisse de l'espérance de vie.\n",
    "\n",
    "On peut également s'intéresser à la présence du Luxembourg dans un cluster à part. En effet, le pays est présent dans le cluster 1 pour chacune des années de notre jeu de données. Lorsqu'on regarde les valeurs de ses colonnes, on observe que le pays a des valeurs plus élevées que tous les autres pays pour le PIB par habitant ainsi que pour l'émission de CO2, ce qui est bien le cas dans la réalité. On peut donc en déduire que le clustering a bien fonctionné pour ce pays.\n",
    "\n",
    "Enfin, on voit que le Qatar se retrouve dans plusieurs clusters mais séparé des autres pays. Etant donné que c'est un pays qui émet beaucoup de CO2, c'est pour cette raison que les points le représentant sont éloignés des autres et se retrouvent dans des clusters à part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut terminer par tester une autre méthode de clustering avec le modèle de mélanges gaussien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste différentes valeurs de nombre de composants pour le modèle de mélange gaussien\n",
    "def test_nombre_composants_bgm(df):\n",
    "    for n_components in range(2, 10):\n",
    "        bgm = BayesianGaussianMixture(n_components=n_components)\n",
    "        bgm.fit(df)\n",
    "        labels = bgm.predict(df)\n",
    "        score = silhouette_score(df, labels)\n",
    "        print(f\"n_components={n_components}, Score de silhouette={score:.2f}\")\n",
    "        \n",
    "test_nombre_composants_bgm(df_normalise)\n",
    "\n",
    "# On applique le modèle de mélange gaussien avec 2 composants\n",
    "bgm = BayesianGaussianMixture(n_components=2)\n",
    "bgm.fit(df_normalise)\n",
    "\n",
    "# On dénormalise les données\n",
    "df_bgm = denormalisation(df_normalise)\n",
    "df_bgm = df_bgm.select_dtypes(include = ['float64'])\n",
    "\n",
    "# On ajoute les clusters au dataframe pour les afficher\n",
    "labels = bgm.predict(df_normalise)\n",
    "df_bgm[\"clusters\"] = labels\n",
    "sns.pairplot(df_bgm, hue=\"clusters\", palette=\"spring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On regarde pour chaque continent la répartition des clusters\n",
    "df_bgm[\"continent\"] = df[\"continent\"]\n",
    "print(\"Nombre de points par cluster :\")\n",
    "print(df_bgm.groupby([\"clusters\"]).size())\n",
    "df_bgm.groupby([\"continent\", \"clusters\"]).size().unstack().plot(kind='bar', stacked=True, colormap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient deux clusters de taille similaire avec le modèle de mélanges gaussien mais cela ne nous apporte pas d'informations supplémentaires par rapport aux autres méthodes de clustering. On imagine que les pays sont répartis en deux clusters en fonction de leur niveau de développement mais cela ne nous apprend rien de plus.\n",
    "\n",
    "En conclusion, on peut dire que les différentes méthodes de clustering nous ont permis de mettre en évidence des groupes de pays qui ont des caractéristiques similaires et qui peuvent être regroupés ensemble. Cela nous a permis de voir que les pays sont répartis en fonction de leur niveau de développement et que les pays les moins développés se retrouvent principalement en Afrique tandis que les pays les plus développés se retrouvent en Europe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage des variables catégorielles\n",
    "\n",
    "On décide de garder KMeans comme méthode de clustering car c'est celle qui nous a donné les meilleurs résultats. On commence par utiliser un objet ColumnTransformer pour transformer les données et prendre en compte cette fois toutes les colonnes pour trouver le nombre optimal de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fonction qui permet d'encoder les variables catégorielles\n",
    "'''\n",
    "def encodage(df):\n",
    "    # On récupère les colonnes numériques et catégorielles\n",
    "    colonnes_numeriques = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    colonnes_objets = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # On crée un objet ColumnTransformer qu'on applique sur le dataframe\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), colonnes_numeriques),\n",
    "            ('cat', OneHotEncoder(), colonnes_objets)\n",
    "        ])\n",
    "    X = preprocessor.fit_transform(df)\n",
    "    return X\n",
    "\n",
    "df_encode = encodage(df_complet_normalise)\n",
    "    \n",
    "meilleur_nombre_classe_kmeans(df_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit avec l'utilisation de la méthode du coude que le nombre optimal de clusters est toujours de 3. On peut donc essayer de faire un clustering avec 3 clusters pour voir si on peut dégager quelque chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans avec 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df_encode)\n",
    "\n",
    "# On ajoute les clusters au dataframe pour les afficher\n",
    "labels = kmeans.labels_\n",
    "df_kmeans = df.copy()\n",
    "df_kmeans[\"cluster\"] = labels\n",
    "sns.pairplot(df_kmeans, hue=\"cluster\", palette=\"spring\")\n",
    "\n",
    "# On regarde pour chaque continent la répartition des clusters\n",
    "df_kmeans[\"continent\"] = df[\"continent\"]\n",
    "print(df_kmeans.groupby([\"continent\", \"cluster\"]).size())\n",
    "df_kmeans.groupby([\"continent\", \"cluster\"]).size().unstack().plot(kind='bar', stacked=True, colormap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a pas de différence majeure dans les résultats obtenus que ce soit en prenant en compte toutes les colonnes ou seulement les colonnes numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction de dimensions\n",
    "\n",
    "On va essayer de réduire la dimension de nos données pour voir si on peut obtenir de meilleurs résultats. On commence par utiliser PCA en cherchant le nombre de composantes qui explique au moins 80% de la variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On teste le nombre de composants pour le PCA\n",
    "pca = PCA()\n",
    "pca.fit(df_normalise)\n",
    "variance_expliquee = pca.explained_variance_ratio_  # Variance expliquée pour chaque composante\n",
    "variance_cumulee = np.cumsum(variance_expliquee)  # Variance cumulée\n",
    "\n",
    "# Affichage de la courbe de variance expliquée\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(variance_cumulee) + 1), variance_cumulee)\n",
    "plt.xlabel('Nombre de composantes')\n",
    "plt.ylabel('Propotion de variance')\n",
    "plt.grid()\n",
    "plt.axhline(y=0.8, linestyle='--', label='80% Variance expliquée')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fixant un seuil à 80%, on trouve que l'on peut garder seulement 2 composantes principales pour garder suffisamment d'informations. On peut donc essayer de faire un clustering avec ce nombre de composantes pour voir si on peut obtenir de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique PCA avec 2 composants\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_normalise)\n",
    "\n",
    "# On cherche le meilleur nombre de clusters pour KMeans\n",
    "meilleur_nombre_classe_kmeans(df_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garder 3 clusters est toujours le meilleur choix pour notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans avec 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df_pca)\n",
    "\n",
    "# On crée un dataframe avec les composantes principales et les clusters\n",
    "clusters = pd.Series(kmeans.predict(df_pca))\n",
    "tableau = pd.DataFrame(df_pca, columns=[\"x\", \"y\"])\n",
    "tableau[\"cluster\"] = clusters\n",
    "\n",
    "# Afficher avec les pays sur le graphique\n",
    "tableau[\"country\"] = df[\"country\"]\n",
    "fig = px.scatter(tableau, x=\"x\", y=\"y\", color=\"cluster\", hover_name=\"country\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode PCA nous permet une meilleure visualisation des clusters obtenus. On retrouve les mêmes clusters que précédemment mais on peut mieux les distinguer.\n",
    "\n",
    "On peut essayer une autre méthode de réduction de dimensions avec t-SNE pour voir si on obtient ces mêmes résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=20)\n",
    "df_tsne = tsne.fit_transform(df_normalise)\n",
    "\n",
    "# Kmeans avec 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df_tsne)\n",
    "clusters = kmeans.labels_\n",
    "df_tsne = pd.DataFrame(df_tsne, columns=['x', 'y'])\n",
    "df_tsne['cluster'] = clusters \n",
    "\n",
    "# Afficher avec les pays sur le graphique\n",
    "df_tsne[\"country\"] = df[\"country\"]\n",
    "fig = px.scatter(df_tsne, x=\"x\", y=\"y\", color=\"cluster\", hover_name=\"country\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse temporelles et spatiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un graphique qui montre l'évolution de l'IDH et du % de services en fonction de l'année\n",
    "df_complet = pd.read_csv('gapminder_data_graphs.csv')\n",
    "graphique = px.scatter(df_complet, x = \"hdi_index\",  y = \"services\", color = \"continent\", animation_frame=\"year\", range_x=[0,1], range_y= [0, 100])\n",
    "graphique.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphique = px.scatter(df_complet, x = \"hdi_index\",  y = \"life_exp\", color = \"continent\", animation_frame=\"year\", range_x=[0,1], range_y= [0, 100])\n",
    "graphique.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Moyennes par année et continent\n",
    "data_temporel = df.groupby(['year', 'continent']).mean(numeric_only=True).reset_index()\n",
    "fig, ax = plt.subplots(5, 1, figsize=(12, 20))\n",
    "sns.lineplot(data=data_temporel, x='year', y='life_exp', hue='continent', ax= ax[0], marker='o')\n",
    "sns.lineplot(data=data_temporel, x='year', y='hdi_index', hue='continent', ax= ax[1], marker='o')\n",
    "sns.lineplot(data=data_temporel, x='year', y='co2_consump', hue='continent', ax= ax[2], marker='o')\n",
    "sns.lineplot(data=data_temporel, x='year', y='gdp', hue='continent', ax= ax[3], marker='o')\n",
    "sns.lineplot(data=data_temporel, x='year', y='services', hue='continent', ax= ax[4], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection d'anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4  # Taille de la fenêtre pour la moyenne mobile\n",
    "deviation_threshold = 2  # Seuil pour la déviation\n",
    "\n",
    "# Fonction pour calculer la moyenne mobile et détecter les anomalies\n",
    "def detect_anomalies(group):\n",
    "    group['moving_avg'] = group['life_exp'].rolling(window=window_size, center=True).mean()\n",
    "    group['deviation'] = group['life_exp'] - group['moving_avg']\n",
    "    group['anomaly'] = (abs(group['deviation']) > deviation_threshold)\n",
    "    return group\n",
    "\n",
    "# Appliquer la fonction à chaque pays\n",
    "data = df.groupby('country').apply(detect_anomalies)\n",
    "\n",
    "anomalies = data[data['anomaly']]\n",
    "print(\"Anomalies détectées :\")\n",
    "anomalies[['country', 'year', 'life_exp', 'moving_avg', 'deviation']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4  # Taille de la fenêtre pour la moyenne mobile\n",
    "deviation_threshold = 3  # Seuil pour la déviation\n",
    "\n",
    "# Fonction pour calculer la moyenne mobile et détecter les anomalies\n",
    "def detect_anomalies(group):\n",
    "    group['moving_avg'] = group['co2_consump'].rolling(window=window_size, center=True).mean()\n",
    "    group['deviation'] = group['co2_consump'] - group['moving_avg']\n",
    "    group['anomaly'] =  (abs(group['deviation']) > deviation_threshold)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Appliquer la fonction à chaque pays\n",
    "data = df.groupby('country').apply(detect_anomalies)\n",
    "\n",
    "anomalies = data[data['anomaly']]\n",
    "print(\"Anomalies détectées :\")\n",
    "anomalies[['country', 'year', 'co2_consump', 'moving_avg', 'deviation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4  # Taille de la fenêtre pour la moyenne mobile\n",
    "deviation_threshold = 2000  # Seuil pour la déviation\n",
    "\n",
    "# Fonction pour calculer la moyenne mobile et détecter les anomalies\n",
    "def detect_anomalies(group):\n",
    "    group['moving_avg'] = group['gdp'].rolling(window=window_size, center=True).mean()\n",
    "    group['deviation'] = group['gdp'] - group['moving_avg']\n",
    "    group['anomaly'] =  (abs(group['deviation']) > deviation_threshold)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Appliquer la fonction à chaque pays\n",
    "data = df.groupby('country').apply(detect_anomalies)\n",
    "\n",
    "anomalies = data[data['anomaly']]\n",
    "print(\"Anomalies détectées :\")\n",
    "anomalies[['country', 'year', 'gdp', 'moving_avg', 'deviation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4  # Taille de la fenêtre pour la moyenne mobile\n",
    "deviation_threshold = 2  # Seuil pour la déviation\n",
    "\n",
    "# Fonction pour calculer la moyenne mobile et détecter les anomalies\n",
    "def detect_anomalies(group):\n",
    "    group['moving_avg'] = group['services'].rolling(window=window_size, center=True).mean()\n",
    "    group['deviation'] = group['services'] - group['moving_avg']\n",
    "    group['anomaly'] =  (abs(group['deviation']) > deviation_threshold)\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Appliquer la fonction à chaque pays\n",
    "data = df.groupby('country').apply(detect_anomalies)\n",
    "\n",
    "anomalies = data[data['anomaly']]\n",
    "print(\"Anomalies détectées :\")\n",
    "anomalies[['country', 'year', 'services', 'moving_avg', 'deviation']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
